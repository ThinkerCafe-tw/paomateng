{
  "id": "snapshot_1761032615823_ujeyr8bse",
  "approvalId": "approval_1761032615821_uzupj21f7",
  "approvalTitle": "Design Document - Railway News Monitor",
  "version": 1,
  "timestamp": "2025-10-21T07:43:35.823Z",
  "trigger": "initial",
  "status": "pending",
  "content": "# Design Document\n\n## Overview\n\nThe Railway News Monitor is a Python-based web scraping and monitoring system designed for academic research. It automatically tracks Taiwan Railway Administration (TRA) announcements, detecting service disruptions and analyzing how crisis communications evolve over time. The system operates in two modes: an initial historical scrape to build a baseline dataset, and continuous monitoring to capture real-time updates and content modifications.\n\nThe architecture emphasizes modularity, reliability, and research-grade data quality. Each component has a single, well-defined responsibility, making the system maintainable and extensible for future research needs.\n\n## Steering Document Alignment\n\n### Technical Standards (tech.md)\n\nNo existing steering documents. This design establishes the following technical standards:\n- **Language**: Python 3.10+ for mature library ecosystem and type hints\n- **Code Style**: PEP 8 compliance, enforced via `black` formatter and `flake8` linter\n- **Type Safety**: Type hints throughout, validated with `mypy`\n- **Error Handling**: Explicit exception handling with structured logging\n- **Testing**: pytest framework with minimum 80% coverage target\n\n### Project Structure (structure.md)\n\nNo existing structure document. This design proposes:\n\n```\nrailway-news-monitor/\n├── src/\n│   ├── scrapers/          # Web scraping components\n│   ├── parsers/           # Content extraction logic\n│   ├── classifiers/       # Classification logic\n│   ├── storage/           # Data persistence\n│   └── orchestrator/      # Main execution control\n├── config/                # Configuration files\n├── data/                  # Output data (master.json)\n├── logs/                  # Application logs\n├── tests/                 # Test suite\n└── requirements.txt       # Python dependencies\n```\n\n## Code Reuse Analysis\n\n### Existing Components to Leverage\n\nThis is a greenfield project with no existing codebase. However, we will leverage mature open-source libraries:\n\n- **requests**: HTTP client for web scraping with built-in retry logic\n- **BeautifulSoup4 + lxml**: HTML parsing with robust error handling\n- **APScheduler**: Job scheduling for monitoring mode\n- **pydantic**: Data validation and settings management\n- **loguru**: Structured logging with rotation\n\n### Integration Points\n\n- **File System**: `master.json` will be the single source of truth, stored in the `data/` directory\n- **External API**: TRA website (read-only, no authentication required)\n- **Future Extensions**: Design allows for future export to databases (PostgreSQL, SQLite) or cloud storage\n\n## Architecture\n\nThe system follows a **pipeline architecture** with clear separation of concerns:\n\n1. **Scraper Layer**: Fetches raw HTML from TRA website\n2. **Parser Layer**: Extracts structured data from HTML\n3. **Classifier Layer**: Tags announcements by category\n4. **Storage Layer**: Persists data to JSON with atomic writes\n5. **Orchestrator Layer**: Coordinates execution and scheduling\n\n### Modular Design Principles\n\n- **Single File Responsibility**: Each module handles one domain (e.g., `list_scraper.py`, `content_parser.py`)\n- **Component Isolation**: Scrapers don't know about storage; parsers don't know about scheduling\n- **Service Layer Separation**: Business logic (classification, extraction) separated from I/O (scraping, storage)\n- **Utility Modularity**: Shared utilities (hash computation, date parsing) in dedicated modules\n\n```mermaid\ngraph TD\n    A[Orchestrator] --> B[List Scraper]\n    A --> C[Detail Scraper]\n    B --> D[HTML Parser]\n    C --> D\n    D --> E[Content Parser]\n    D --> F[Classifier]\n    E --> G[Storage Manager]\n    F --> G\n    G --> H[master.json]\n    A --> I[Scheduler]\n    I --> A\n```\n\n## Components and Interfaces\n\n### Component 1: List Scraper (`scrapers/list_scraper.py`)\n\n- **Purpose**: Fetches announcement list pages from TRA website\n- **Interfaces**:\n  - `scrape_page(page_num: int) -> List[AnnouncementListItem]`\n  - `scrape_all_pages() -> List[AnnouncementListItem]`\n- **Dependencies**: `requests`, `BeautifulSoup4`\n- **Reuses**: Standard library `urllib.parse` for URL construction\n- **Error Handling**: Retries (3x with exponential backoff), logs failures, continues on error\n\n### Component 2: Detail Scraper (`scrapers/detail_scraper.py`)\n\n- **Purpose**: Fetches individual announcement detail pages\n- **Interfaces**:\n  - `scrape_detail(detail_url: str) -> DetailPageContent`\n  - `compute_hash(html: str) -> str`\n- **Dependencies**: `requests`, `hashlib`\n- **Reuses**: Shared `HTTPClient` utility for consistent request handling\n- **Error Handling**: Network timeout (30s), malformed HTML detection, retry logic\n\n### Component 3: Content Parser (`parsers/content_parser.py`)\n\n- **Purpose**: Extracts structured data from HTML using regex and keyword matching\n- **Interfaces**:\n  - `parse(html: str) -> ExtractedData`\n  - `extract_report_version(html: str) -> Optional[str]`\n  - `extract_predicted_time(html: str) -> Optional[datetime]`\n  - `extract_affected_lines(html: str) -> List[str]`\n- **Dependencies**: `re`, `dateparser`, `BeautifulSoup4`\n- **Reuses**: Shared `RegexPatterns` configuration class\n- **Error Handling**: Returns `None` for failed extractions, logs parsing errors\n\n### Component 4: Classifier (`classifiers/announcement_classifier.py`)\n\n- **Purpose**: Categorizes announcements using keyword matching\n- **Interfaces**:\n  - `classify(title: str, content: str) -> Classification`\n  - `extract_event_group_id(title: str, publish_date: str) -> str`\n- **Dependencies**: `re`, `config/keywords.yaml`\n- **Reuses**: Shared keyword configuration loaded at startup\n- **Error Handling**: Defaults to `\"General_Operation\"` category if no match\n\n### Component 5: Storage Manager (`storage/json_storage.py`)\n\n- **Purpose**: Manages atomic read/write operations to master.json\n- **Interfaces**:\n  - `load() -> List[Announcement]`\n  - `save(data: List[Announcement]) -> None`\n  - `append_version(news_no: str, version: VersionEntry) -> None`\n  - `add_announcement(announcement: Announcement) -> None`\n- **Dependencies**: `json`, `filelock`\n- **Reuses**: `pydantic` models for data validation\n- **Error Handling**: File locking prevents corruption, backup on write failure\n\n### Component 6: Orchestrator (`orchestrator/main.py`)\n\n- **Purpose**: Coordinates historical scrape and monitoring workflows\n- **Interfaces**:\n  - `run_historical_scrape() -> None`\n  - `run_monitoring_cycle() -> None`\n  - `start_monitoring(interval_minutes: int) -> None`\n- **Dependencies**: All other components, `APScheduler`\n- **Reuses**: Configuration from `config/settings.py`\n- **Error Handling**: Catches all exceptions, logs to file, continues operation\n\n## Data Models\n\n### AnnouncementListItem\n\n```python\nfrom pydantic import BaseModel\n\nclass AnnouncementListItem(BaseModel):\n    news_no: str           # Unique ID from URL\n    title: str             # Announcement title\n    publish_date: str      # Format: \"YYYY/MM/DD\"\n    detail_url: str        # Full URL to detail page\n```\n\n### ExtractedData\n\n```python\nfrom typing import Optional, List\nfrom pydantic import BaseModel\nfrom datetime import datetime\n\nclass ExtractedData(BaseModel):\n    report_version: Optional[str] = None          # \"1\", \"2\", \"第3發\"\n    event_type: Optional[str] = None              # \"Typhoon\", \"Heavy_Rain\", etc.\n    status: Optional[str] = None                  # \"Suspended\", \"Partial_Operation\", etc.\n    affected_lines: List[str] = []                # [\"西部幹線\", \"東部幹線\"]\n    affected_stations: List[str] = []             # [\"二水\", \"林內\"]\n    predicted_resumption_time: Optional[datetime] = None  # ISO 8601\n    actual_resumption_time: Optional[datetime] = None     # ISO 8601\n```\n\n### Classification\n\n```python\nclass Classification(BaseModel):\n    category: str                  # \"Disruption_Suspension\", \"Disruption_Update\", etc.\n    keywords: List[str]            # Matched keywords\n    event_group_id: str            # \"20251021_Pingxi_Rain\"\n```\n\n### VersionEntry\n\n```python\nclass VersionEntry(BaseModel):\n    scraped_at: datetime           # ISO 8601 format\n    content_html: str              # Raw HTML content\n    content_hash: str              # \"md5:a1b2c3d4...\"\n    extracted_data: Optional[ExtractedData] = None\n```\n\n### Announcement\n\n```python\nclass Announcement(BaseModel):\n    id: str                        # Same as news_no\n    title: str\n    publish_date: str\n    detail_url: str\n    classification: Classification\n    version_history: List[VersionEntry]\n```\n\n## Error Handling\n\n### Error Scenarios\n\n1. **Network Timeout/Connection Error**\n   - **Handling**: Retry 3 times with exponential backoff (1s, 2s, 4s). Log error if all retries fail. Skip announcement and continue.\n   - **User Impact**: Logged as warning. Missing announcements flagged in summary report.\n\n2. **Malformed HTML/Parsing Error**\n   - **Handling**: Log error with announcement ID. Set `extracted_data = None`. Continue processing.\n   - **User Impact**: Announcement stored with raw HTML but no structured data. Researcher can manually review.\n\n3. **JSON Write Failure**\n   - **Handling**: Create timestamped backup of current `master.json`. Attempt write again. If fails, halt execution.\n   - **User Impact**: System stops to prevent data loss. Admin must resolve disk/permission issue.\n\n4. **Hash Collision (unlikely)**\n   - **Handling**: Log warning. Treat as content change to be safe (false positive).\n   - **User Impact**: Extra version recorded. Better than missing a real change.\n\n5. **Regex Extraction Failure**\n   - **Handling**: Return `None` for that specific field. Log at DEBUG level (not an error).\n   - **User Impact**: Field appears as `null` in JSON. Expected for non-disruption announcements.\n\n6. **Rate Limiting by TRA Server**\n   - **Handling**: Respect `Retry-After` header if present. Otherwise, exponential backoff up to 60s.\n   - **User Impact**: Scraping slows down but completes successfully.\n\n## Testing Strategy\n\n### Unit Testing\n\n- **Approach**: pytest with fixtures for HTML samples\n- **Key components to test**:\n  - `content_parser.py`: Test all extraction functions with real TRA HTML samples\n  - `announcement_classifier.py`: Test keyword matching with edge cases\n  - `json_storage.py`: Test atomic writes, concurrent access with file locking\n  - `detail_scraper.py`: Test hash computation consistency\n\n**Sample Tests**:\n```python\ndef test_extract_predicted_time_typical():\n    html = \"<p>預計於今日19:00恢復行駛</p>\"\n    result = extract_predicted_time(html)\n    assert result.hour == 19\n    assert result.minute == 0\n\ndef test_classify_disruption():\n    title = \"平溪線因豪雨暫停營運\"\n    category = classify(title, \"\")\n    assert category.category == \"Disruption_Suspension\"\n    assert \"豪雨\" in category.keywords\n```\n\n### Integration Testing\n\n- **Approach**: Test full pipeline with mock HTTP responses\n- **Key flows to test**:\n  - Historical scrape: Mock TRA list pages → verify JSON structure\n  - Monitoring cycle: Mock content change → verify new version appended\n  - Error recovery: Mock network error → verify retry logic\n\n**Sample Test**:\n```python\n@pytest.fixture\ndef mock_tra_server(httpserver):\n    httpserver.expect_request(\"/tip911/newsList?page=0\").respond_with_json(...)\n\ndef test_historical_scrape_integration(mock_tra_server):\n    orchestrator.run_historical_scrape()\n    data = storage.load()\n    assert len(data) > 0\n    assert data[0].version_history[0].extracted_data is not None\n```\n\n### End-to-End Testing\n\n- **Approach**: Run against TRA staging/production with limited scope (1-2 pages)\n- **User scenarios to test**:\n  1. **First-time setup**: Run historical scrape, verify `master.json` created\n  2. **Monitoring detection**: Manually modify a real announcement, verify system detects change\n  3. **Long-running stability**: Run monitoring for 24 hours, verify no memory leaks or crashes\n\n**Validation**:\n- Compare extracted data against manual review of announcements\n- Verify `predicted_resumption_time` parsing accuracy (>95% target)\n- Confirm no data loss during concurrent writes\n\n## Technology Stack\n\n### Core Dependencies\n\n```\nrequests==2.31.0          # HTTP client\nbeautifulsoup4==4.12.0    # HTML parsing\nlxml==5.0.0               # Fast HTML parser\npydantic==2.5.0           # Data validation\nAPScheduler==3.10.0       # Job scheduling\nloguru==0.7.0             # Logging\npython-dateutil==2.8.0    # Date parsing\nfilelock==3.13.0          # File locking for JSON\n```\n\n### Development Dependencies\n\n```\npytest==7.4.0             # Testing framework\npytest-cov==4.1.0         # Coverage reporting\npytest-httpserver==1.0.0  # Mock HTTP server\nblack==23.12.0            # Code formatting\nflake8==7.0.0             # Linting\nmypy==1.7.0               # Type checking\n```\n\n## Configuration Management\n\nAll configuration will be externalized to `config/settings.yaml`:\n\n```yaml\nscraper:\n  base_url: \"https://www.railway.gov.tw/tra-tip-web/tip/tip009/tip911\"\n  user_agent: \"TRA News Monitor (Research Project - Professor Lin)\"\n  request_timeout: 30\n  retry_attempts: 3\n  rate_limit_delay: 1.0  # seconds between requests\n\nmonitoring:\n  interval_minutes: 5\n  max_pages_to_check: 2\n\nstorage:\n  output_file: \"data/master.json\"\n  backup_dir: \"data/backups\"\n  pretty_print: true\n\nlogging:\n  level: \"INFO\"\n  log_file: \"logs/railway_monitor.log\"\n  rotation: \"10 MB\"\n```\n\n## Performance Considerations\n\n- **Historical Scrape**: Estimated 500 pages × 20 announcements/page × 1s/request = ~3 hours (with rate limiting)\n- **Monitoring Cycle**: 2 pages × 20 announcements × 50% new/changed × 1s/request = ~20 seconds\n- **Memory Usage**: In-memory processing of one page at a time, estimated <100MB RAM\n- **Storage Growth**: ~1MB per 1000 announcements, minimal growth rate\n\n## Security and Ethics\n\n- **robots.txt Compliance**: Check and respect TRA's robots.txt directives\n- **Rate Limiting**: 1 request/second to avoid overwhelming TRA servers\n- **User-Agent**: Clear identification for TRA administrators to contact if needed\n- **No Authentication Bypass**: Only access publicly available data\n- **Data Privacy**: No personal information collected (all data is public announcements)\n",
  "fileStats": {
    "size": 14308,
    "lines": 373,
    "lastModified": "2025-10-21T07:43:23.449Z"
  },
  "comments": []
}