{
  "id": "snapshot_1761033059875_kxx6c6y50",
  "approvalId": "approval_1761033054608_pk9wqal2g",
  "approvalTitle": "Tasks Document - Railway News Monitor (20 Implementation Tasks)",
  "version": 2,
  "timestamp": "2025-10-21T07:50:59.875Z",
  "trigger": "approved",
  "status": "pending",
  "content": "# Tasks Document\n\n## Phase 1: Project Setup and Infrastructure\n\n- [ ] 1. Initialize Python project structure and dependencies\n  - Files: `requirements.txt`, `config/settings.yaml`, `src/__init__.py`\n  - Create project directory structure (src/, config/, data/, logs/, tests/)\n  - Initialize requirements.txt with all dependencies\n  - Create settings.yaml with configuration parameters\n  - Purpose: Establish project foundation and dependency management\n  - _Leverage: Python standard library pathlib for directory creation_\n  - _Requirements: Design - Technology Stack, Configuration Management_\n  - _Prompt: Implement the task for spec railway-news-monitor, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Python DevOps Engineer with expertise in project structure and dependency management | Task: Initialize complete Python project structure following the design document's project structure specification, create requirements.txt with exact versions from design (requests==2.31.0, beautifulsoup4==4.12.0, etc.), and create config/settings.yaml with all configuration parameters from the design. Mark this task as in-progress in tasks.md before starting, then mark as complete when done. | Restrictions: Use exact dependency versions from design document, do not add unlisted dependencies, ensure directory structure matches design exactly | _Leverage: Python pathlib for directory creation, YAML for configuration | _Requirements: Design Document - Technology Stack, Configuration Management | Success: All directories created successfully, requirements.txt contains all specified dependencies with correct versions, settings.yaml contains all configuration from design document, project can be initialized with `pip install -r requirements.txt`_\n\n- [ ] 2. Create Pydantic data models\n  - Files: `src/models/__init__.py`, `src/models/announcement.py`\n  - Implement all Pydantic models: AnnouncementListItem, ExtractedData, Classification, VersionEntry, Announcement\n  - Add validation rules and type hints\n  - Purpose: Define type-safe data structures for the entire system\n  - _Leverage: Pydantic BaseModel for validation_\n  - _Requirements: Requirement 6 (Data Persistence), Requirement 7 (Structured Data Extraction), Design - Data Models_\n  - _Prompt: Implement the task for spec railway-news-monitor, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Python Developer specializing in data modeling and Pydantic | Task: Create all Pydantic models from the design document (AnnouncementListItem, ExtractedData, Classification, VersionEntry, Announcement) in src/models/announcement.py, implementing all fields with proper types and validation rules. ExtractedData must include all 7 fields (report_version, event_type, status, affected_lines, affected_stations, predicted_resumption_time, actual_resumption_time). Mark this task as in-progress in tasks.md before starting, then mark as complete when done. | Restrictions: Must use exact field names from design, all datetime fields must be timezone-aware, maintain Optional types where specified, do not add extra fields | _Leverage: Pydantic BaseModel, datetime module for timezone handling | _Requirements: Requirement 6 (version_history structure), Requirement 7 (extracted_data fields) | Success: All models defined with correct field types, validation works correctly, models can serialize to/from JSON, datetime fields handle ISO 8601 format_\n\n## Phase 2: Core Utilities\n\n- [ ] 3. Create HTTP client utility with retry logic\n  - Files: `src/utils/__init__.py`, `src/utils/http_client.py`\n  - Implement HTTPClient class with retry logic (3 attempts, exponential backoff)\n  - Add rate limiting (1 request/second)\n  - Configure User-Agent and timeout settings\n  - Purpose: Provide robust HTTP client for all scraping operations\n  - _Leverage: requests library with Session for connection pooling_\n  - _Requirements: Requirement 7 (Error Handling), Design - List Scraper, Detail Scraper_\n  - _Prompt: Implement the task for spec railway-news-monitor, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Backend Developer with expertise in HTTP clients and error handling | Task: Implement HTTPClient class in src/utils/http_client.py with retry logic (3 attempts, exponential backoff 1s/2s/4s), rate limiting (1 req/sec), custom User-Agent from config, and 30-second timeout. Include methods get(url) and get_with_retry(url). Mark this task as in-progress in tasks.md before starting, then mark as complete when done. | Restrictions: Must respect rate_limit_delay from settings.yaml, do not exceed retry_attempts from config, must handle network errors gracefully, log all retry attempts | _Leverage: requests.Session for connection pooling, time.sleep for rate limiting, tenacity or custom retry decorator | _Requirements: Requirement 7 AC.1 (retry 3 times with exponential backoff), Design - Error Handling Scenario 1 | Success: HTTP requests retry correctly on failure, rate limiting prevents requests faster than 1/sec, all network errors are caught and logged, timeout works correctly at 30s_\n\n- [ ] 4. Create hash computation utility\n  - Files: `src/utils/hash_utils.py`\n  - Implement compute_hash(html: str) -> str function\n  - Use MD5 algorithm with consistent encoding (UTF-8)\n  - Return format: \"md5:a1b2c3d4...\"\n  - Purpose: Enable content change detection\n  - _Leverage: Python hashlib standard library_\n  - _Requirements: Requirement 1 AC.3, Requirement 3 (Content Change Detection), Design - Detail Scraper_\n  - _Prompt: Implement the task for spec railway-news-monitor, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Python Developer with expertise in cryptographic hashing | Task: Implement compute_hash(html: str) -> str function in src/utils/hash_utils.py using MD5 algorithm with UTF-8 encoding, returning format \"md5:<hexdigest>\". Ensure consistent hashing for same input. Mark this task as in-progress in tasks.md before starting, then mark as complete when done. | Restrictions: Must use MD5 (not SHA256) for performance, must handle UTF-8 encoding correctly, output format must be exactly \"md5:<hash>\", function must be deterministic | _Leverage: hashlib.md5() from Python standard library | _Requirements: Requirement 1 AC.3 (compute content hash), Requirement 3 AC.1 (use MD5 or SHA256), Requirement 2 AC.4 (hash comparison) | Success: Function returns consistent hash for same input, handles Chinese characters correctly, output format matches specification, hash computation is fast (<10ms per page)_\n\n- [ ] 5. Create date parsing utility\n  - Files: `src/utils/date_utils.py`\n  - Implement parse_tra_date(date_str: str) -> str for \"YYYY/MM/DD\" format\n  - Implement parse_resumption_time(text: str) -> Optional[datetime] with regex patterns\n  - Handle Chinese date/time formats (e.g., \"今日19:00\", \"8月13日12時\")\n  - Purpose: Extract and normalize temporal information\n  - _Leverage: python-dateutil, datetime, regex_\n  - _Requirements: Requirement 7 AC.3 (extract predicted_resumption_time), Design - Content Parser_\n  - _Prompt: Implement the task for spec railway-news-monitor, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Python Developer with expertise in date parsing and regex | Task: Implement date parsing utilities in src/utils/date_utils.py: parse_tra_date(date_str) for \"YYYY/MM/DD\" to ISO format, and parse_resumption_time(text) using regex to extract Chinese time expressions like \"今日19:00\", \"8月13日12時前\", \"明日凌晨\". Return timezone-aware datetime in Asia/Taipei. Mark this task as in-progress in tasks.md before starting, then mark as complete when done. | Restrictions: Must handle relative dates (\"今日\", \"明日\"), must return timezone-aware datetime (Asia/Taipei), return None if parsing fails, do not raise exceptions | _Leverage: python-dateutil.parser, datetime.timezone, re module for regex | _Requirements: Requirement 7 AC.3 (extract predicted_resumption_time field), extracted_data must use ISO 8601 format | Success: Parses \"YYYY/MM/DD\" correctly, extracts Chinese time expressions accurately, handles relative dates, returns timezone-aware datetime, gracefully returns None on parsing failure_\n\n## Phase 3: Scraping Components\n\n- [ ] 6. Implement List Scraper\n  - Files: `src/scrapers/__init__.py`, `src/scrapers/list_scraper.py`\n  - Implement scrape_page(page_num: int) -> List[AnnouncementListItem]\n  - Implement scrape_all_pages() -> List[AnnouncementListItem] for historical scrape\n  - Parse list page HTML to extract news_no, title, publish_date, detail_url\n  - Purpose: Fetch announcement lists from TRA website\n  - _Leverage: HTTPClient from utils, BeautifulSoup4 for HTML parsing, AnnouncementListItem model_\n  - _Requirements: Requirement 1 AC.1-2 (iterate through list pages, extract metadata), Design - List Scraper_\n  - _Prompt: Implement the task for spec railway-news-monitor, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Python Web Scraping Engineer with BeautifulSoup expertise | Task: Implement ListScraper class in src/scrapers/list_scraper.py with methods scrape_page(page_num) and scrape_all_pages(). Parse HTML to extract newsNo from href (e.g., \"newsNo=8ae4cac...\"), title from anchor text, publish_date, and construct full detail_url. Use base_url from config. Iterate until empty page. Mark this task as in-progress in tasks.md before starting, then mark as complete when done. | Restrictions: Must use HTTPClient utility (do not create new requests), must handle empty pages gracefully, extract newsNo from URL parameter correctly, validate AnnouncementListItem before returning | _Leverage: src/utils/http_client.py, BeautifulSoup4 with lxml parser, src/models/announcement.py | _Requirements: Requirement 1 AC.1 (iterate pages from page=0), AC.2 (extract title, publish_date, detail_url, newsNo) | Success: Successfully scrapes list pages, extracts all required fields correctly, handles pagination until no data, returns validated AnnouncementListItem objects, respects rate limiting_\n\n- [ ] 7. Implement Detail Scraper\n  - Files: `src/scrapers/detail_scraper.py`\n  - Implement scrape_detail(detail_url: str) -> Tuple[str, str] returning (html_content, hash)\n  - Extract detail page content and compute hash\n  - Handle malformed HTML gracefully\n  - Purpose: Fetch announcement detail pages with change detection\n  - _Leverage: HTTPClient, hash_utils.compute_hash, BeautifulSoup4_\n  - _Requirements: Requirement 1 AC.3 (compute hash), Requirement 2 AC.3 (re-fetch detail page), Design - Detail Scraper_\n  - _Prompt: Implement the task for spec railway-news-monitor, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Python Web Scraping Engineer with expertise in HTML parsing and error handling | Task: Implement DetailScraper class in src/scrapers/detail_scraper.py with scrape_detail(detail_url) method returning tuple (content_html: str, content_hash: str). Extract main content div, compute hash using hash_utils, handle malformed HTML. Mark this task as in-progress in tasks.md before starting, then mark as complete when done. | Restrictions: Must use HTTPClient utility, must use hash_utils.compute_hash (do not reimpute), handle BeautifulSoup parsing errors gracefully, extract only the announcement content (not entire page) | _Leverage: src/utils/http_client.py, src/utils/hash_utils.py, BeautifulSoup4 for content extraction | _Requirements: Requirement 1 AC.3 (compute content hash), Requirement 2 AC.3 (re-fetch for existing newsNo), Requirement 7 AC.3 (malformed HTML handling) | Success: Fetches detail pages successfully, extracts content HTML correctly, computes hash consistently, handles malformed HTML without crashing, returns clean HTML content_\n\n## Phase 4: Content Processing\n\n- [ ] 8. Implement Content Parser (Structured Data Extraction)\n  - Files: `src/parsers/__init__.py`, `src/parsers/content_parser.py`, `config/regex_patterns.yaml`\n  - Implement parse(html: str) -> ExtractedData with all 7 fields\n  - Create regex patterns for: report_version, event_type, status, lines, stations, times\n  - Handle parsing failures gracefully (return None for fields)\n  - Purpose: Extract structured data from HTML for research analysis\n  - _Leverage: ExtractedData model, date_utils for time parsing, regex library_\n  - _Requirements: Requirement 7 (Structured Data Extraction) - ALL acceptance criteria, Design - Content Parser_\n  - _Prompt: Implement the task for spec railway-news-monitor, first run spec-workflow-guide to get the workflow guide then implement the task: Role: NLP Engineer with expertise in regex pattern matching and Chinese text processing | Task: Implement ContentParser class in src/parsers/content_parser.py with parse(html) method extracting all 7 fields: report_version (regex for \"第\\d+報\", \"第\\d+發\"), event_type (keywords: 颱風->Typhoon, 豪雨->Heavy_Rain, 地震->Earthquake, etc.), status (停駛->Suspended, 部分->Partial_Operation, etc.), affected_lines (西部幹線, 東部幹線, etc.), affected_stations (extract station names), predicted_resumption_time (use date_utils), actual_resumption_time. Store regex patterns in config/regex_patterns.yaml. Return null for failed extractions. Mark this task as in-progress in tasks.md before starting, then mark as complete when done. | Restrictions: Must return ExtractedData model, all failed extractions must be None (not empty string), use date_utils.parse_resumption_time for time fields, log parsing errors at DEBUG level only, do not raise exceptions on parse failure | _Leverage: src/models/announcement.py ExtractedData, src/utils/date_utils.py, re module, YAML config for patterns | _Requirements: Requirement 7 AC.1 (execute parsing on version_history), AC.2 (use RegEx), AC.3 (populate all 7 fields), AC.4-5 (null on failure, log errors) | Success: Extracts all 7 fields correctly from real TRA HTML samples, predicted_resumption_time extraction accuracy >90%, handles missing data gracefully with None, returns valid ExtractedData model, regex patterns are maintainable in YAML config_\n\n- [ ] 9. Implement Classifier\n  - Files: `src/classifiers/__init__.py`, `src/classifiers/announcement_classifier.py`, `config/keywords.yaml`\n  - Implement classify(title: str, content: str) -> Classification\n  - Implement extract_event_group_id(title: str, publish_date: str) -> str\n  - Load keyword rules from config/keywords.yaml\n  - Purpose: Categorize announcements and group related events\n  - _Leverage: Classification model, regex for event name extraction_\n  - _Requirements: Requirement 4 (Announcement Classification), Requirement 5 (Event Grouping), Design - Classifier_\n  - _Prompt: Implement the task for spec railway-news-monitor, first run spec-workflow-guide to get the workflow guide then implement the task: Role: ML Engineer specializing in text classification and keyword matching | Task: Implement AnnouncementClassifier class in src/classifiers/announcement_classifier.py with classify(title, content) returning Classification (category, matched keywords, event_group_id). Categories: Disruption_Suspension (keywords: 停駛, 暫停營運, 中斷, 落石, 出軌), Disruption_Update (第2報, 第3報, 第N報), Disruption_Resumption (恢復行駛, 恢復通車, 已排除), General_Operation (default). Implement extract_event_group_id extracting event name + date format \"YYYYMMDD_EventName\". Store keywords in config/keywords.yaml. Mark this task as in-progress in tasks.md before starting, then mark as complete when done. | Restrictions: Must scan both title and content, return all matched keywords (not just first), default to General_Operation if no match, event_group_id format must be \"YYYYMMDD_EventName\", load keywords from YAML config | _Leverage: src/models/announcement.py Classification, re module for event name extraction, YAML config for keywords | _Requirements: Requirement 4 AC.1-6 (classification logic), Requirement 5 AC.1-4 (event grouping) | Success: Correctly categorizes announcements by keyword matching, extracts event group IDs accurately, returns all matched keywords, loads configuration from YAML, handles edge cases (no keywords matched)_\n\n## Phase 5: Data Persistence\n\n- [ ] 10. Implement Storage Manager with atomic writes\n  - Files: `src/storage/__init__.py`, `src/storage/json_storage.py`\n  - Implement load() -> List[Announcement]\n  - Implement save(data: List[Announcement]) -> None with file locking\n  - Implement append_version(news_no: str, version: VersionEntry) -> None\n  - Implement add_announcement(announcement: Announcement) -> None\n  - Create backup mechanism on write failures\n  - Purpose: Provide reliable JSON persistence with data integrity guarantees\n  - _Leverage: Announcement model, filelock library, json module_\n  - _Requirements: Requirement 6 (Data Persistence), Requirement 7 AC.4 (JSON write failures), Design - Storage Manager_\n  - _Prompt: Implement the task for spec railway-news-monitor, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Backend Developer with expertise in file I/O and data integrity | Task: Implement JSONStorage class in src/storage/json_storage.py with methods: load() reading master.json and returning List[Announcement], save(data) with atomic writes using file locking (filelock), append_version(news_no, version) for adding new version_history entries, add_announcement(announcement) for new announcements. Create timestamped backup to data/backups/ on write failure. Output JSON with indentation (pretty_print from config). Mark this task as in-progress in tasks.md before starting, then mark as complete when done. | Restrictions: Must use filelock to prevent concurrent write corruption, create backup before every write attempt, validate Announcement models before saving, maintain UTF-8 encoding, use indent=2 for pretty printing, preserve file if save fails | _Leverage: src/models/announcement.py Announcement model, filelock library for locking, json module, pathlib for file operations | _Requirements: Requirement 6 AC.1-5 (JSON structure and operations), Requirement 7 AC.4 (preserve state on failure), Design - Reliability (atomic writes) | Success: JSON loads and saves correctly with Pydantic validation, file locking prevents corruption during concurrent access, backups created on failures, JSON is human-readable with indentation, data integrity maintained across crashes_\n\n## Phase 6: Orchestration\n\n- [ ] 11. Implement Historical Scrape orchestrator\n  - Files: `src/orchestrator/__init__.py`, `src/orchestrator/historical_scraper.py`\n  - Implement run_historical_scrape() function\n  - Integrate: ListScraper -> DetailScraper -> ContentParser -> Classifier -> Storage\n  - Add progress logging and error handling\n  - Purpose: Execute initial full scrape of all TRA announcements\n  - _Leverage: All scraper, parser, classifier, storage components_\n  - _Requirements: Requirement 1 (Historical Data Collection) - all ACs, Design - Orchestrator_\n  - _Prompt: Implement the task for spec railway-news-monitor, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Integration Engineer with expertise in data pipeline orchestration | Task: Implement run_historical_scrape() in src/orchestrator/historical_scraper.py orchestrating: (1) ListScraper.scrape_all_pages() to get all announcements, (2) For each announcement: DetailScraper.scrape_detail() to get HTML+hash, (3) ContentParser.parse() to extract structured data, (4) Classifier.classify() for categorization, (5) Create Announcement object with VersionEntry containing extracted_data, (6) JSONStorage.add_announcement(). Add progress logging every 10 announcements. Handle errors per Requirement 7 AC.2 (log and continue). Mark this task as in-progress in tasks.md before starting, then mark as complete when done. | Restrictions: Must follow exact pipeline sequence, do not stop on individual announcement failures, log progress with announcement count and ETA, validate data models at each step, ensure extracted_data is included in version_history | _Leverage: All implemented components (scrapers, parsers, classifier, storage), loguru for logging | _Requirements: Requirement 1 AC.1-5 (historical scrape process), Requirement 7 AC.1-2 (error handling) | Success: Successfully scrapes all pages sequentially, extracts and stores all required data including extracted_data, handles errors gracefully without stopping, provides clear progress logging, creates valid master.json with complete structure_\n\n- [ ] 12. Implement Monitoring orchestrator with scheduling\n  - Files: `src/orchestrator/monitor.py`\n  - Implement run_monitoring_cycle() function for incremental updates\n  - Implement start_monitoring(interval_minutes: int) with APScheduler\n  - Detect new announcements and content changes via hash comparison\n  - Purpose: Continuously monitor for new/updated announcements\n  - _Leverage: All components, APScheduler for scheduling, storage for hash comparison_\n  - _Requirements: Requirement 2 (Incremental Monitoring), Requirement 3 (Content Change Detection), Design - Orchestrator_\n  - _Prompt: Implement the task for spec railway-news-monitor, first run spec-workflow-guide to get the workflow guide then implement the task: Role: DevOps Engineer with expertise in job scheduling and incremental data processing | Task: Implement run_monitoring_cycle() in src/orchestrator/monitor.py: (1) Scrape pages 0-1 only, (2) For each announcement, check if newsNo exists in storage, (3) If new: run full extraction pipeline (same as historical), (4) If exists: scrape detail, compute new hash, compare with latest version_history hash, (5) If hash differs: run full extraction and append new VersionEntry with extracted_data to version_history, (6) If hash same: skip. Implement start_monitoring(interval_minutes) using APScheduler.BackgroundScheduler. Mark this task as in-progress in tasks.md before starting, then mark as complete when done. | Restrictions: Must only scrape max_pages_to_check from config (default 2), must compare with most recent version_history entry's hash, must run full extraction (parser + classifier) on content changes, scheduler must handle errors without stopping, log all detected changes | _Leverage: All components, APScheduler for scheduling, JSONStorage for loading existing data | _Requirements: Requirement 2 AC.1-6 (monitoring process), Requirement 3 AC.1-5 (change detection), Design - Orchestrator monitoring workflow | Success: Detects new announcements correctly, identifies content changes via hash comparison, appends new versions with extracted_data, runs on schedule without crashes, logs detected changes clearly, respects configured monitoring interval_\n\n- [ ] 13. Create main entry point with CLI\n  - Files: `src/main.py`\n  - Implement CLI with argparse: --mode [historical|monitor]\n  - Add --interval option for monitoring mode\n  - Initialize logging configuration\n  - Purpose: Provide user-friendly command-line interface\n  - _Leverage: orchestrator functions, argparse, loguru_\n  - _Requirements: Design - Usability, Configuration Management_\n  - _Prompt: Implement the task for spec railway-news-monitor, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Python Developer with expertise in CLI design and application entry points | Task: Create src/main.py with argparse CLI: --mode {historical,monitor} (required), --interval (default from config, for monitor mode), --config (path to settings.yaml, default config/settings.yaml). Initialize loguru logging (level and file from config with rotation). Call historical_scraper.run_historical_scrape() or monitor.start_monitoring(). Add helpful --help descriptions. Mark this task as in-progress in tasks.md before starting, then mark as complete when done. | Restrictions: Must validate --mode argument, --interval only applies to monitor mode, load config before any operations, configure logging before any component initialization, handle KeyboardInterrupt gracefully for monitor mode | _Leverage: argparse for CLI, loguru for logging, PyYAML for config loading, src/orchestrator modules | _Requirements: Design - Configuration Management (externalized config), Usability (clear CLI) | Success: CLI accepts both modes correctly, loads configuration from YAML, initializes logging properly, provides helpful --help text, handles Ctrl+C gracefully in monitor mode_\n\n## Phase 7: Testing\n\n- [ ] 14. Write unit tests for utilities\n  - Files: `tests/__init__.py`, `tests/test_utils.py`\n  - Test http_client retry logic and rate limiting\n  - Test hash_utils consistency and UTF-8 handling\n  - Test date_utils parsing with various Chinese formats\n  - Purpose: Ensure utility functions are reliable\n  - _Leverage: pytest, pytest-mock for HTTP mocking_\n  - _Requirements: Design - Testing Strategy (Unit Testing)_\n  - _Prompt: Implement the task for spec railway-news-monitor, first run spec-workflow-guide to get the workflow guide then implement the task: Role: QA Engineer with expertise in pytest and unit testing | Task: Create comprehensive unit tests in tests/test_utils.py for: (1) http_client: test retry on failure (3 attempts), test rate limiting (time between requests), test timeout, (2) hash_utils: test consistency (same input = same hash), test UTF-8/Chinese characters, (3) date_utils: test parse_tra_date, test parse_resumption_time with samples (\"今日19:00\", \"8月13日12時前\", \"明日凌晨\"), test None on parse failure. Use pytest-mock or responses for HTTP mocking. Mark this task as in-progress in tasks.md before starting, then mark as complete when done. | Restrictions: Mock external HTTP calls (do not hit real servers), test both success and failure scenarios, use parametrize for multiple test cases, assert specific error types, aim for >90% coverage of utils | _Leverage: pytest framework, pytest-mock or responses for mocking, freezegun for datetime mocking | _Requirements: Design - Unit Testing section | Success: All utility functions tested with good coverage, HTTP retry logic verified with mocks, hash consistency validated, date parsing tested with real Chinese examples, tests run quickly (<5s) and independently_\n\n- [ ] 15. Write unit tests for parsers and classifiers\n  - Files: `tests/test_parser.py`, `tests/test_classifier.py`, `tests/fixtures/sample_html.py`\n  - Create real TRA HTML samples as fixtures\n  - Test ContentParser extraction accuracy for all 7 fields\n  - Test Classifier categorization and event grouping\n  - Purpose: Validate extraction and classification logic\n  - _Leverage: pytest fixtures, real TRA HTML samples_\n  - _Requirements: Design - Testing Strategy (Unit Testing for parsers)_\n  - _Prompt: Implement the task for spec railway-news-monitor, first run spec-workflow-guide to get the workflow guide then implement the task: Role: QA Engineer with expertise in NLP testing and fixture management | Task: Create tests/test_parser.py testing ContentParser with real TRA HTML samples (store in tests/fixtures/sample_html.py): test extraction of all 7 fields (report_version, event_type, status, affected_lines, affected_stations, predicted_resumption_time, actual_resumption_time), test null on parse failure, test Chinese character handling. Create tests/test_classifier.py: test all categories (Disruption_Suspension, Update, Resumption, General_Operation), test keyword matching, test event_group_id format, test default category. Use pytest fixtures for HTML samples. Mark this task as in-progress in tasks.md before starting, then mark as complete when done. | Restrictions: Use real HTML samples from TRA website (copy 5+ examples), test all classification categories, verify event_group_id format \"YYYYMMDD_EventName\", test edge cases (empty content, no keywords), aim for >85% extraction accuracy validation | _Leverage: pytest fixtures, BeautifulSoup for fixture creation, real TRA announcements as test data | _Requirements: Design - Unit Testing (parser and classifier testing) | Success: Parser tests validate all 7 field extractions with real HTML, classifier correctly categorizes test cases, event grouping produces correct IDs, tests catch regression in extraction logic, fixtures represent diverse announcement types_\n\n- [ ] 16. Write integration tests for full pipeline\n  - Files: `tests/test_integration.py`\n  - Test historical scrape with mocked HTTP server (pytest-httpserver)\n  - Test monitoring cycle with simulated content changes\n  - Verify JSON output structure and data integrity\n  - Purpose: Validate end-to-end data flow\n  - _Leverage: pytest-httpserver for mocking TRA website, JSONStorage_\n  - _Requirements: Design - Integration Testing_\n  - _Prompt: Implement the task for spec railway-news-monitor, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Integration Test Engineer with expertise in API mocking and E2E testing | Task: Create tests/test_integration.py with: (1) test_historical_scrape_integration using pytest-httpserver to mock TRA list/detail pages, verify complete pipeline creates valid master.json with extracted_data, (2) test_monitoring_detects_new_announcement mocking new announcement appearing, (3) test_monitoring_detects_content_change mocking same newsNo with different HTML, verify new VersionEntry appended with different hash. Validate JSON structure matches Requirement 6. Use temporary directory for test outputs. Mark this task as in-progress in tasks.md before starting, then mark as complete when done. | Restrictions: Use pytest-httpserver (not real HTTP requests), validate Pydantic models in assertions, use tmp_path fixture for test data directory, verify extracted_data is present in all version_history entries, clean up test files after each test | _Leverage: pytest-httpserver for HTTP mocking, tmp_path fixture, JSONStorage, all pipeline components | _Requirements: Design - Integration Testing, Requirement 6 (validate JSON structure), Requirement 2-3 (monitoring and change detection) | Success: Integration tests verify full pipeline with mocked HTTP, JSON output structure is validated against requirements, new announcements and content changes are detected correctly, tests run in isolation with clean state, all tests pass consistently_\n\n## Phase 8: Documentation and Finalization\n\n- [ ] 17. Create README and usage documentation\n  - Files: `README.md`, `docs/USAGE.md`, `docs/DATA_SCHEMA.md`\n  - Write installation instructions\n  - Document CLI usage and examples\n  - Document JSON output schema with examples\n  - Explain configuration options\n  - Purpose: Enable researchers to use the system\n  - _Leverage: Markdown, requirements from spec documents_\n  - _Requirements: Design - Usability section_\n  - _Prompt: Implement the task for spec railway-news-monitor, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Technical Writer with expertise in developer documentation | Task: Create comprehensive documentation: (1) README.md with project overview, installation (pip install -r requirements.txt), quick start (historical and monitor modes), configuration overview, (2) docs/USAGE.md with detailed CLI examples, configuration file explanation, common workflows, (3) docs/DATA_SCHEMA.md documenting master.json structure with example Announcement object showing all fields including extracted_data. Use code blocks for examples. Mark this task as in-progress in tasks.md before starting, then mark as complete when done. | Restrictions: Use clear examples with actual commands, document all CLI flags, explain all config/settings.yaml options, provide JSON schema example from design document, keep language accessible for researchers (not just developers) | _Leverage: Requirements and Design documents for reference, Markdown formatting | _Requirements: Design - Usability, Configuration Management sections | Success: README provides clear installation and quick start, USAGE.md covers all CLI options with examples, DATA_SCHEMA.md accurately documents JSON structure with complete example, documentation is accessible to non-expert users_\n\n- [ ] 18. Add logging and monitoring instrumentation\n  - Files: `src/utils/logger.py` (configure loguru)\n  - Configure structured logging with rotation\n  - Add metrics logging (announcements processed, changes detected, errors)\n  - Create log format from config (level, file, rotation)\n  - Purpose: Provide operational visibility\n  - _Leverage: loguru library_\n  - _Requirements: Design - Usability (logs provide visibility), Configuration Management_\n  - _Prompt: Implement the task for spec railway-news-monitor, first run spec-workflow-guide to get the workflow guide then implement the task: Role: DevOps Engineer with expertise in logging and observability | Task: Configure loguru in src/utils/logger.py with setup_logging() function reading config/settings.yaml logging section (level, log_file, rotation). Add structured logging throughout orchestrators: log start/end of scrape cycles, announcements processed count, new announcements detected, content changes detected, parsing/classification errors, HTTP failures. Use loguru context binding for structured fields. Mark this task as in-progress in tasks.md before starting, then mark as complete when done. | Restrictions: Read all logging config from settings.yaml, use appropriate log levels (DEBUG for parsing details, INFO for progress, WARNING for retries, ERROR for failures), enable rotation (default \"10 MB\"), add timestamps in ISO format, do not log sensitive data | _Leverage: loguru library, config/settings.yaml logging section | _Requirements: Design - Usability (clear visibility into progress and errors), Configuration Management | Success: Logging configured from settings.yaml, log rotation works correctly, structured logs include useful context (announcement ID, operation type), log levels are appropriate, logs are human-readable and parseable_\n\n- [ ] 19. Final integration testing and bug fixes\n  - Files: Multiple (bug fixes as needed)\n  - Run full historical scrape against real TRA website (limited scope: 5 pages)\n  - Run monitoring cycle for 1 hour, verify change detection\n  - Fix any discovered bugs\n  - Validate data quality (spot-check extracted_data accuracy)\n  - Purpose: Ensure production readiness\n  - _Leverage: All components, real TRA website_\n  - _Requirements: Design - End-to-End Testing, All Requirements_\n  - _Prompt: Implement the task for spec railway-news-monitor, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Senior QA Engineer with expertise in system validation and production readiness | Task: Execute final validation: (1) Run historical scrape with max 5 pages against real TRA website, verify master.json created with valid structure, (2) Manually spot-check 10 announcements: verify extracted_data accuracy (predicted_resumption_time, event_type, status), (3) Run monitoring mode for 1 hour, verify no crashes, (4) Review logs for errors/warnings, fix critical bugs, (5) Validate Requirement 7 AC (all 7 fields extracted, null on failure), (6) Document any known limitations. Mark this task as in-progress in tasks.md before starting, then mark as complete when done. | Restrictions: Limit historical scrape to 5 pages to avoid overloading TRA servers, respect rate limiting, do not modify core logic unless bugs found, document all discovered issues, validate against requirements document, obtain real HTML samples for future tests | _Leverage: Complete system, real TRA website, requirements and design docs for validation | _Requirements: All Requirements (comprehensive validation), Design - E2E Testing | Success: System successfully scrapes real TRA data, extracted_data fields are >85% accurate for disruption announcements, monitoring runs stably for 1 hour, no critical bugs remain, data quality is suitable for research, all requirements are met_\n\n- [ ] 20. Create deployment guide and production checklist\n  - Files: `docs/DEPLOYMENT.md`, `scripts/deploy.sh`\n  - Write deployment instructions (local, server, Docker)\n  - Create activation script for monitoring mode\n  - Document backup and recovery procedures\n  - Provide troubleshooting guide\n  - Purpose: Enable production deployment\n  - _Leverage: Shell scripting, systemd or cron for scheduling_\n  - _Requirements: Design - Reliability (resumption support), Usability_\n  - _Prompt: Implement the task for spec railway-news-monitor, first run spec-workflow-guide to get the workflow guide then implement the task: Role: DevOps Engineer with expertise in deployment automation and production operations | Task: Create docs/DEPLOYMENT.md with: (1) Local deployment (venv setup, config, running historical scrape), (2) Server deployment (systemd service or cron for monitoring mode), (3) Docker deployment (optional Dockerfile), (4) Backup strategy (data/ and logs/ directories), (5) Recovery procedures (restoring from backup), (6) Troubleshooting (common errors and solutions). Create scripts/deploy.sh for automated setup (install deps, create directories, validate config). Mark this task as in-progress in tasks.md before starting, then mark as complete when done. | Restrictions: Document must be production-ready (not just dev environment), provide actual systemd service file example, include log rotation setup, document config validation steps, provide monitoring health check commands | _Leverage: Shell scripting, systemd, cron, venv for Python isolation | _Requirements: Design - Reliability section, Usability | Success: Deployment guide is complete and production-ready, deploy.sh automates setup successfully, systemd/cron examples work correctly, backup/recovery procedures are clear and tested, troubleshooting guide addresses common issues_\n",
  "fileStats": {
    "size": 37986,
    "lines": 222,
    "lastModified": "2025-10-21T07:50:42.432Z"
  },
  "comments": []
}