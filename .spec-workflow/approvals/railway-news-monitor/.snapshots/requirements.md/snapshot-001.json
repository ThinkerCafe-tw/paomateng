{
  "id": "snapshot_1761031905643_2u9gynsu2",
  "approvalId": "approval_1761031905639_a4t0x0u88",
  "approvalTitle": "Requirements Document - Railway News Monitor",
  "version": 1,
  "timestamp": "2025-10-21T07:31:45.643Z",
  "trigger": "initial",
  "status": "pending",
  "content": "# Requirements Document\n\n## Introduction\n\nThe Railway News Monitor system is an automated web scraping and monitoring tool designed to track and analyze Taiwan Railway Administration (TRA) public announcements. This system serves academic research by capturing the complete lifecycle of service disruption announcements (caused by typhoons, earthquakes, heavy rain, etc.), tracking their publication patterns, update frequencies, and content evolution over time.\n\nThe system addresses critical research needs by:\n- Automatically detecting and tagging announcements related to train service interruptions\n- Tracking both new announcements (Report #1, #2, #3...) and in-place content modifications\n- Recording precise timestamps for temporal analysis of response patterns\n\n## Alignment with Product Vision\n\nThis system enables Professor Lin's research on crisis communication patterns by providing comprehensive, timestamped data on how TRA communicates service disruptions to the public. The automated monitoring eliminates manual tracking efforts and ensures complete data capture during critical events.\n\n## Requirements\n\n### Requirement 1: Historical Data Collection\n\n**User Story:** As a researcher, I want to collect all existing TRA announcements from the archive, so that I can establish a baseline dataset for historical analysis.\n\n#### Acceptance Criteria\n\n1. WHEN the initial scrape is triggered THEN the system SHALL iterate through all list pages starting from page=0 until no data is returned\n2. WHEN processing each list page THEN the system SHALL extract title, publish_date, detail_url, and newsNo for each announcement\n3. WHEN visiting each detail page THEN the system SHALL compute a content hash (MD5 or SHA256) of the HTML content\n4. WHEN storing initial data THEN the system SHALL record scraped_at timestamp in ISO 8601 format\n5. WHEN completing historical scrape THEN the system SHALL save all data to master.json in the specified structure\n\n### Requirement 2: Incremental Monitoring\n\n**User Story:** As a researcher, I want the system to continuously monitor new announcements, so that I can capture real-time updates during service disruption events.\n\n#### Acceptance Criteria\n\n1. WHEN the monitoring job executes THEN the system SHALL fetch only the first 1-2 list pages to optimize performance\n2. WHEN encountering a newsNo not in the database THEN the system SHALL classify it as a new announcement and perform full data extraction\n3. WHEN encountering an existing newsNo THEN the system SHALL re-fetch the detail page and compute a new content hash\n4. IF the new hash differs from the latest version_history hash THEN the system SHALL append a new version record with new content, hash, and timestamp\n5. IF the new hash matches the existing hash THEN the system SHALL skip processing and continue to the next announcement\n6. WHEN the monitoring job is configured THEN the system SHALL execute at intervals of 5 minutes (configurable)\n\n### Requirement 3: Content Change Detection\n\n**User Story:** As a researcher, I want to detect when TRA modifies existing announcements, so that I can analyze how crisis communications evolve over time.\n\n#### Acceptance Criteria\n\n1. WHEN computing content hash THEN the system SHALL use MD5 or SHA256 algorithm on the detail page HTML content\n2. WHEN a content change is detected THEN the system SHALL preserve all previous versions in the version_history array\n3. WHEN appending a new version THEN the system SHALL include scraped_at, content_html, and content_hash fields\n4. WHEN comparing hashes THEN the system SHALL use the most recent version_history entry as the reference\n5. IF the title is modified THEN the system SHALL optionally update the top-level title field\n\n### Requirement 4: Announcement Classification\n\n**User Story:** As a researcher, I want announcements automatically tagged by category, so that I can filter for service-disruption-related events without manual review.\n\n#### Acceptance Criteria\n\n1. WHEN processing any announcement THEN the system SHALL scan both title and content_html for classification keywords\n2. WHEN keywords match disruption patterns ['停駛', '暫停營運', '中斷', '落石', '出軌'] THEN the system SHALL set category to \"Disruption_Suspension\"\n3. WHEN keywords match update patterns ['第2報', '第3報', '第N報'] THEN the system SHALL set category to \"Disruption_Update\"\n4. WHEN keywords match resumption patterns ['恢復行駛', '恢復通車', '已排除'] THEN the system SHALL set category to \"Disruption_Resumption\"\n5. WHEN keywords match weather patterns ['颱風', '豪雨', '地震'] THEN the system SHALL add weather-related tags\n6. WHEN classification is complete THEN the system SHALL store category and matched keywords in the classification object\n\n### Requirement 5: Event Grouping\n\n**User Story:** As a researcher, I want related announcements (Report #1, #2, #3...) grouped under a single event ID, so that I can reconstruct the complete timeline of a service disruption.\n\n#### Acceptance Criteria\n\n1. WHEN processing a classified announcement THEN the system SHALL attempt to extract event name from the title (e.g., \"平溪線豪雨\", \"樺加沙颱風\")\n2. WHEN an event name is identified THEN the system SHALL combine it with publish_date to generate event_group_id (format: \"YYYYMMDD_EventName\")\n3. WHEN storing the announcement THEN the system SHALL include event_group_id in the classification object\n4. IF multiple announcements share the same event_group_id THEN researchers SHALL be able to filter and analyze them as a cohesive event sequence\n\n### Requirement 6: Data Persistence\n\n**User Story:** As a researcher, I want all announcement data stored in a structured JSON format, so that I can perform programmatic analysis and export.\n\n#### Acceptance Criteria\n\n1. WHEN storing data THEN the system SHALL use a JSON file named master.json\n2. WHEN structuring each announcement THEN the system SHALL include fields: id, title, publish_date, detail_url, classification, and version_history\n3. WHEN storing version_history THEN each entry SHALL include scraped_at, content_html, and content_hash\n4. WHEN updating existing announcements THEN the system SHALL append to version_history without overwriting previous versions\n5. WHEN writing to JSON THEN the system SHALL maintain valid JSON structure and UTF-8 encoding\n\n### Requirement 7: Error Handling and Resilience\n\n**User Story:** As a system operator, I want the scraper to handle network errors gracefully, so that temporary failures don't corrupt the dataset.\n\n#### Acceptance Criteria\n\n1. WHEN a network request fails THEN the system SHALL retry up to 3 times with exponential backoff\n2. IF all retries fail THEN the system SHALL log the error and continue processing other announcements\n3. WHEN encountering malformed HTML THEN the system SHALL log the error and skip that specific announcement\n4. WHEN JSON write operations fail THEN the system SHALL preserve the previous valid state\n5. WHEN the monitoring job encounters errors THEN it SHALL continue on the next scheduled execution\n\n## Non-Functional Requirements\n\n### Code Architecture and Modularity\n- **Single Responsibility Principle**: Separate modules for scraping, classification, hashing, and storage\n- **Modular Design**: Extractor (list/detail page parsing), Classifier (keyword matching), Storage Manager (JSON operations)\n- **Dependency Management**: Minimize coupling between scraping logic and storage format\n- **Clear Interfaces**: Define contracts between scraper, classifier, and storage components\n\n### Performance\n- Historical scrape SHALL complete within reasonable time (estimated 5-10 minutes for 500 pages)\n- Monitoring cycle SHALL complete within 1-2 minutes to maintain 5-minute intervals\n- Hash computation SHALL use optimized algorithms (MD5 preferred for speed)\n- JSON file operations SHALL use incremental writes to avoid loading entire file into memory\n\n### Security\n- The system SHALL respect robots.txt directives\n- Request rate SHALL be limited to avoid overwhelming TRA servers (recommended: 1 request per second)\n- User-Agent header SHALL identify the scraper appropriately\n\n### Reliability\n- The system SHALL maintain data integrity during crashes (atomic JSON writes)\n- The system SHALL support resumption of historical scrape if interrupted\n- Monitoring jobs SHALL be idempotent (re-running produces same results)\n\n### Usability\n- Configuration SHALL be externalized (scrape intervals, page limits, keyword lists)\n- Logs SHALL provide clear visibility into scraping progress and errors\n- Output JSON SHALL be human-readable (formatted with indentation)\n",
  "fileStats": {
    "size": 8652,
    "lines": 131,
    "lastModified": "2025-10-21T07:31:37.268Z"
  },
  "comments": []
}